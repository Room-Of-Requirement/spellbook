# Servers
apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: k3s-server
  namespace: system-upgrade
  labels:
    k3s-upgrade: server
spec:
  concurrency: 1
  channel: https://update.k3s.io/v1-release/channels/stable
  nodeSelector:
    matchExpressions:
    # - {key: k3s-upgrade, operator: Exists}
    - {key: k3s-upgrade, operator: NotIn, values: ["disabled", "false"]}
    - {key: kubernetes.io/hostname, operator: Exists}
    - {key: k3os.io/mode, operator: DoesNotExist}
    - {key: node-role.kubernetes.io/master, operator: In, values: ["true"]}
  serviceAccountName: system-upgrade
  cordon: true
#  drain:
#    force: true
  upgrade:
    image: docker.io/rancher/k3s-upgrade

---

# Agents
apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: k3s-agent
  namespace: system-upgrade
  labels:
    k3s-upgrade: agent
spec:
  concurrency: 1 # in general, this should be the number of workers - 1
  channel: https://update.k3s.io/v1-release/channels/stable
  nodeSelector:
    matchExpressions:
    # - {key: k3s-upgrade, operator: Exists}
    - {key: k3s-upgrade, operator: NotIn, values: ["disabled", "false"]}
    - {key: kubernetes.io/hostname, operator: Exists}
    - {key: k3os.io/mode, operator: DoesNotExist}
    - {key: node-role.kubernetes.io/master, operator: NotIn, values: ["true"]}
  tolerations:
  - key: "RaspberryPi"
    operator: "Exists"
    effect: "NoSchedule"
  serviceAccountName: system-upgrade
  prepare:
    # Since v0.5.0-m1 SUC will use the resolved version of the plan for the tag on the prepare container.
    # image: rancher/k3s-upgrade:v1.17.4-k3s1
    image: docker.io/rancher/k3s-upgrade
    args: ["prepare", "k3s-server"]
  drain:
    force: true
    skipWaitForDeleteTimeout: 60 # set this to prevent upgrades from hanging on small clusters since k8s v1.18
  upgrade:
    image: docker.io/rancher/k3s-upgrade
