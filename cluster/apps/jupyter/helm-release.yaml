apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: jupyter
spec:
  interval: 1m
  chart:
    spec:
      # renovate: registryUrl=https://jupyterhub.github.io/helm-chart/
      chart: jupyterhub
      version: 1.1.3
      sourceRef:
        kind: HelmRepository
        name: jupyterhub
        namespace: flux-system
      interval: 1m
  values:
    # hub relates to the hub pod, responsible for running JupyterHub, its configured
    # Authenticator class KubeSpawner, and its configured Proxy class
    # ConfigurableHTTPProxy. KubeSpawner creates the user pods, and
    # ConfigurableHTTPProxy speaks with the actual ConfigurableHTTPProxy server in
    # the proxy pod.
    hub:
      config:
        Authenticator:
          auto_login: true
          admin_users:
          - jmmaloney4
        JupyterHub:
          admin_access: true
          authenticator_class: github
        GitHubOAuthenticator:
          client_id: ${JUPYTERHUB_CLIENT_ID}
          client_secret: ${JUPYTERHUB_CLIENT_SECRET}
          oauth_callback_url: https://lab.roomofrequirement.xyz/hub/oauth_callback
          allowed_organizations:
          - "Room-Of-Requirement"
          scope:
          - read:user
          - read:org
      baseUrl: /
      db:
        type: sqlite-pvc
        pvc:
          accessModes:
          - ReadWriteMany
          storage: 5Gi
          storageClassName: cephfs
      image:
        name: jupyterhub/k8s-hub
        tag: "set-by-chartpress"
      resources: {}

    rbac:
      enabled: true

    # proxy relates to the proxy pod, the proxy-public service, and the autohttps
    # pod and proxy-http service.
    proxy:
      secretToken:
      annotations: {}
      deploymentStrategy:
        ## type: Recreate
        ## - JupyterHub's interaction with the CHP proxy becomes a lot more robust
        ##   with this configuration. To understand this, consider that JupyterHub
        ##   during startup will interact a lot with the k8s service to reach a
        ##   ready proxy pod. If the hub pod during a helm upgrade is restarting
        ##   directly while the proxy pod is making a rolling upgrade, the hub pod
        ##   could end up running a sequence of interactions with the old proxy pod
        ##   and finishing up the sequence of interactions with the new proxy pod.
        ##   As CHP proxy pods carry individual state this is very error prone. One
        ##   outcome when not using Recreate as a strategy has been that user pods
        ##   have been deleted by the hub pod because it considered them unreachable
        ##   as it only configured the old proxy pod but not the new before trying
        ##   to reach them.
        type: Recreate
        ## rollingUpdate:
        ## - WARNING:
        ##   This is required to be set explicitly blank! Without it being
        ##   explicitly blank, k8s will let eventual old values under rollingUpdate
        ##   remain and then the Deployment becomes invalid and a helm upgrade would
        ##   fail with an error like this:
        ##
        ##     UPGRADE FAILED
        ##     Error: Deployment.apps "proxy" is invalid: spec.strategy.rollingUpdate: Forbidden: may not be specified when strategy `type` is 'Recreate'
        ##     Error: UPGRADE FAILED: Deployment.apps "proxy" is invalid: spec.strategy.rollingUpdate: Forbidden: may not be specified when strategy `type` is 'Recreate'
        rollingUpdate:
      # service relates to the proxy-public service
      service:
        type: ClusterIP
      # chp relates to the proxy pod, which is responsible for routing traffic based
      # on dynamic configuration sent from JupyterHub to CHP's REST API.
      chp:
        image:
          name: jupyterhub/configurable-http-proxy
          tag: 4.5.0 # https://github.com/jupyterhub/configurable-http-proxy/releases
        resources: {}
      # traefik relates to the autohttps pod, which is responsible for TLS
      # termination when proxy.https.type=letsencrypt.
      traefik:
        image:
          name: traefik
          tag: v2.4.11 # ref: https://hub.docker.com/_/traefik?tab=tags
        hsts:
          includeSubdomains: false
          preload: false
          maxAge: 15724800 # About 6 months
        resources: {}
      secretSync:
        image:
          name: jupyterhub/k8s-secret-sync
          tag: "set-by-chartpress"
          pullPolicy:
          pullSecrets: []
        resources: {}
      https:
        enabled: false

    # singleuser relates to the configuration of KubeSpawner which runs in the hub
    # pod, and its spawning of user pods such as jupyter-myusername.
    singleuser:
      networkTools:
        image:
          name: jupyterhub/k8s-network-tools
          tag: "set-by-chartpress"
          pullPolicy:
          pullSecrets: []
      storage:
        type: dynamic
        extraLabels: {}
        extraVolumes: []
        extraVolumeMounts: []
        static:
          pvcName:
          subPath: "{username}"
        capacity: 10Gi
        homeMountPath: /home/jovyan
        dynamic:
          storageClass: ceph-rbd-hdd-ec
          pvcNameTemplate: claim-{username}{servername}
          volumeNameTemplate: volume-{username}{servername}
          storageAccessModes: [ReadWriteOnce]
      image:
        name: jupyterhub/k8s-singleuser-sample
        tag: "set-by-chartpress"
      startTimeout: 300
      cpu:
        limit:
        guarantee:
      memory:
        limit:
        guarantee: 1G
      extraResource:
        limits: {}
        guarantees: {}
      cmd: jupyterhub-singleuser
      defaultUrl:
      extraPodConfig: {}
      profileList: []

    # scheduling relates to the user-scheduler pods and user-placeholder pods.
    scheduling:
      userScheduler:
        enabled: true
        replicas: 2
        logLevel: 4
        # plugins ref: https://kubernetes.io/docs/reference/scheduling/config/#scheduling-plugins-1
        image:
          # IMPORTANT: Bumping the minor version of this binary should go hand in
          #            hand with an inspection of the user-scheduelrs RBAC resources
          #            that we have forked.
          name: k8s.gcr.io/kube-scheduler
          tag: v1.19.13 # ref: https://github.com/kubernetes/website/blob/main/content/en/releases/patch-releases.md
      userPlaceholder:
        enabled: true
        image:
          name: k8s.gcr.io/pause
          # tag's can be updated by inspecting the output of the command:
          # gcloud container images list-tags k8s.gcr.io/pause --sort-by=~tags
          #
          # If you update this, also update prePuller.pause.image.tag
          tag: "3.5"

    # prePuller relates to the hook|continuous-image-puller DaemonsSets
    prePuller:
      # hook relates to the hook-image-awaiter Job and hook-image-puller DaemonSet
      hook:
        enabled: true
        pullOnlyOnChanges: true
        # image and the configuration below relates to the hook-image-awaiter Job
        image:
          name: jupyterhub/k8s-image-awaiter
          tag: "set-by-chartpress"
          pullPolicy:
          pullSecrets: []
      pause:
        image:
          name: k8s.gcr.io/pause
          # tag's can be updated by inspecting the output of the command:
          # gcloud container images list-tags k8s.gcr.io/pause --sort-by=~tags
          #
          # If you update this, also update scheduling.userPlaceholder.image.tag
          tag: "3.5"
          pullPolicy:
          pullSecrets: []

    ingress:
      enabled: false
      annotations: {}
      hosts: []
      pathSuffix:
      pathType: Prefix
      tls: []

    # cull relates to the jupyterhub-idle-culler service, responsible for evicting
    # inactive singleuser pods.
    #
    # The configuration below, except for enabled, corresponds to command-line flags
    # for jupyterhub-idle-culler as documented here:
    # https://github.com/jupyterhub/jupyterhub-idle-culler#as-a-standalone-script
    #
    cull:
      enabled: true
      users: false # --cull-users
      removeNamedServers: false # --remove-named-servers
      timeout: 3600 # --timeout
      every: 600 # --cull-every
      concurrency: 10 # --concurrency
      maxAge: 0 # --max-age
